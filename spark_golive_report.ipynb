{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import avro.schema\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.window as window\n",
    "from pyspark.sql import *\n",
    "\n",
    "import pickle\n",
    "import os.path\n",
    "from datetime import datetime, timedelta, date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "import h5py\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg') # Must be before importing matplotlib.pyplot or pylab!\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib.dates import DateFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":/data/util/python3.6/site-packages:/usr/lib/python3.6/site-packages/\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import sys\n",
    "import os\n",
    "\n",
    "#RTFReport for python3\n",
    "sys.path.insert(0, '/nfsshare/RFORT/AuditLogExtracts/pgm//04_standard_reports_output/PyRTF')\n",
    "sys.path.insert(0, '/nfsshare/RFORT/AuditLogExtracts/pgm//04_standard_reports_output/')\n",
    "from RTFReport import RTFReport\n",
    "\n",
    "#Need to set environment variables in order to make pyspark work in Python3.6\n",
    "spark_home=os.environ['SPARK_HOME']\n",
    "print(os.environ['PYTHONPATH'])\n",
    "#os.environ['LD_LIBRARY_PATH']=\"/usr/lib64/:/usr/lib/:/nfsshare/apps1/dsp_engine/dsp_engine_20160707/lib/\"#+os.environ['LD_LIBRARY_PATH']\n",
    "os.environ['PYTHONPATH']= spark_home+\"/python/lib/py4j-0.10.4-src.zip\"#+os.environ['PYTHONPATH']\n",
    "os.environ['PYSPARK_PYTHON']=\"/data/util/python3.6/bin/python3.6\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON']=\"/data/util/python3.6/bin/python3.6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "super_dir='/home/lijli06/Work/DFM_SPARK'\n",
    "org_ruleset_hash_path=super_dir+\"/extn_hashes/special_org_ruleset_hash.pickle\"\n",
    "with open(org_ruleset_hash_path,'rb') as handle:\n",
    "    org_ruleset_hashdict=pickle.load(handle)\n",
    "  \n",
    "def sal_process_orgname(s):\n",
    "    #This function is for ruleset mapping\n",
    "    if s not in org_ruleset_hashdict:\n",
    "        return s\n",
    "    else:\n",
    "        return org_ruleset_hashdict[s] \n",
    "\n",
    "sal_process_orgname_udf=F.udf(sal_process_orgname, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GoLiveReport:\n",
    "    def __init__ (self, name, bins=None, description=None,\n",
    "            decimals=2, dateType=False, tableColPercent=None,\n",
    "            colNames=[\"bin lower bound\", \"Count\", \"Percent\", \"Cumulative\", \"Cum_Pct\", \"PctLeft\"],\n",
    "            sortByFreq=False, sortHighToLow=False,            \n",
    "            plot=True, plotCounts=False, plotCum=False, plotInverseCum=False, \n",
    "            plotPoints=False, plotXLog=False, plotYLog=False, \n",
    "            plotXRange=None, plotYRange=None,\n",
    "            plotXLabel=None, plotYLabel=None\n",
    "            ):\n",
    "        \"\"\"\n",
    "            Description: A super class for generating a golive report. All the metrics appearing\n",
    "                        in the report are subclasses to this class. This class has the method to\n",
    "                        load the job configuration file, validate the given date range, load data\n",
    "                        from HDFS for the given date range and orgs, bin field values in general,\n",
    "                        and output rtf report\n",
    "            Purpose: Define a high-level class to include general methods and attributes\n",
    "            Parameters:\n",
    "                name: string; used for the title of generated table and plots\n",
    "                bins: None (default), list or iterable; the bins for binning a field\n",
    "                description: None (default) or list of strings; the description for a table\n",
    "                decimals: integer (default: 2); how many decimals for float in the table display\n",
    "                dateType: False (default) or True; if True, the bins are date type and it requires special care\n",
    "                colNames: list of strings (optional); column headers for the table\n",
    "                tableColPercent: None (default) or list of integers; the percentage of width for each column of the table\n",
    "                sortByFreq: False (default) or True (reserve for future use)\n",
    "                sortHightToLow: False (default) or True; if True, sort the bins from high to low\n",
    "                plot: True (default) or False; if True, include a plot in addition to the generated table\n",
    "                plotCounts: False (default) or True; if True, plot the raw transaction counts\n",
    "                plotCum: False (default) or True; if True, plot the cumulative percentage. When both plotCounts\n",
    "                        and plotCum are False, plot the percentage\n",
    "                plotInverseCum: False (default) or True; if an additional plot is made for inverse cumulative percentage\n",
    "                plotPoints: False (default) or True; if True, use a dot as the marker; or else, plain line plots\n",
    "                plotXLog: False (default) or True; if True, set the xaxis to log scale otherwise linear scale\n",
    "                plotYLog: False (default) or True; if True, set the yaxis to log scale otherwise linear scale\n",
    "                plotXRange: None (default), list of numbers, or iterable; use to determine range of xaxis and the xticks\n",
    "                plotYRange: None (default), list of numbers, or iterable; use to determine range of yaxis and the yticks\n",
    "                plotXLabel: None (default) or string; label for xaxis\n",
    "                plotYLabel: None (default) or string; label for yaxis\n",
    "        \"\"\"\n",
    "        \n",
    "        self.value_dist = {}\n",
    "        self.nCalls = None\n",
    "        self.name = name\n",
    "        self.bin_lower_bound = bins \n",
    "        \n",
    "        self.colNames = colNames\n",
    "        self.tableColPercent = tableColPercent\n",
    "\n",
    "        self.description = description\n",
    "        self.decimals = decimals\n",
    "\n",
    "        self.sortByFreq = sortByFreq\n",
    "        self.sortHighToLow = sortHighToLow\n",
    "        self.plot = plot\n",
    "        self.plotPoints = plotPoints\n",
    "        self.plotCum = plotCum\n",
    "        self.plotInverseCum = plotInverseCum\n",
    "        self.plotXLog = plotXLog\n",
    "        self.plotXRange = plotXRange\n",
    "        self.plotYLog = plotYLog\n",
    "        self.plotYRange = plotYRange\n",
    "        if plotXLabel == None:\n",
    "            self.plotXLabel = name\n",
    "        else:\n",
    "            self.plotXLabel = plotXLabel\n",
    "        self.plotYLabel = plotYLabel\n",
    "        \n",
    "        self.plotCounts = plotCounts\n",
    "        self.dateType = dateType\n",
    "        \n",
    "        self.extra_dates = None\n",
    "        \n",
    "    def load_job_config(self, file_path):\n",
    "        \"\"\"\n",
    "            Description: read the json file for the job/task configurations\n",
    "            Purpose: parse the json file given by the file_path to get the date range, \n",
    "                    data log, issuer groups, and issuer banks.\n",
    "            Parameter:\n",
    "                file_path: the path to the json file that has the job/task configuration\n",
    "        \"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            print('The provided job configuration file does not exist!')\n",
    "        tasks = json.load(open(file_path))\n",
    "        start_date=tasks['Job 1']['StartDate']\n",
    "        end_date=tasks['Job 1']['EndDate']\n",
    "        self.start_date = datetime.strptime(start_date, '%Y-%m-%d').date()\n",
    "        self.end_date = datetime.strptime(end_date, '%Y-%m-%d').date()\n",
    "        \n",
    "        log_label=tasks['Job 1']['Log']\n",
    "        if log_label != 'sysauditlog': #for golive report, only use sysauditlog\n",
    "            self.log_label = 'sysauditlog'\n",
    "        else:\n",
    "            self.log_label = log_label\n",
    "            \n",
    "        self.issuer_groups=tasks['Job 1']['BankGroups'] #e.g. ['NW', 'HSBC']\n",
    "        self.issuer_sets=tasks['Job 1']['BankSets'] #e.g. [['NW-DEBIT', 'NW-CREDIT'], ['HSBCDEBIT', 'HSBCCREDIT']]\n",
    "        \n",
    "        self.dateInterval = tasks['Job 1']['DateInterval'] #integer\n",
    "        self.dateUnit = tasks['Job 1']['DateUnit'] #one of 'days', 'weeks', 'months', 'quarters', or 'years'\n",
    "        \n",
    "    def validate_date_range(self):\n",
    "        \"\"\"\n",
    "            Description: Examine the date range given the start date, date interval, date unit, and end date\n",
    "            Purpose: Parse the date range determined by the given start date, \n",
    "                      date interval, date unit, and end date. The data are only available\n",
    "                      for certain time period (e.g. sysauditlog is available from 6 months\n",
    "                      ago to 6 days ago). \n",
    "                     The given start date and end date may not fit well the date interval and date unit, \n",
    "                      the end date is adjusted if possible.\n",
    "            Parameters:\n",
    "                None\n",
    "        \"\"\"\n",
    "        \n",
    "        #Sanity check of the date\n",
    "        if self.end_date < self.start_date:\n",
    "            print(\"End date is before start date!\")\n",
    "            sys.exit(1)\n",
    "        if self.start_date < date.today() - relativedelta(months=6):\n",
    "            print(\"Start date is more than 6 months ago from today. Data of last 6 months are available!\")\n",
    "            sys.exit(2)\n",
    "            \n",
    "        if self.end_date > date.today() - relativedelta(days=6): #the available data is 6 days ago\n",
    "            print(\"End date cannot be within last 6 days!\")\n",
    "            self.end_date = date.today() - relativedelta(days=6)\n",
    "            print('Adjust the end date to yesterday, {:s}'.format(self.end_date.strftime('%Y-%m-%d')))\n",
    "\n",
    "        #Make sure the start date, end date and date interval are appropriately set\n",
    "        dateUnit = self.dateUnit.lower()\n",
    "        dateInterval = self.dateInterval\n",
    "        if dateUnit=='weeks':\n",
    "            dateUnit = 'days'\n",
    "            dateInterval = self.dateInterval*7\n",
    "        if dateUnit=='quarters':\n",
    "            dateUnit = 'months'\n",
    "            dateInterval = self.dateInterval*3\n",
    "        \n",
    "        if dateUnit=='days': #has to be in days\n",
    "            delta = self.end_date - self.start_date \n",
    "            maxDelta = date.today()-relativedelta(days=6) - self.start_date\n",
    "        else:\n",
    "            delta = relativedelta(self.end_date, self.start_date)\n",
    "            maxDelta = relativedelta(date.today()-relativedelta(days=6), self.start_date)\n",
    "        \n",
    "        reqInterval=np.ceil(getattr(delta, dateUnit)/dateInterval)*dateInterval \\\n",
    "                    if getattr(delta, dateUnit)>0 else dateInterval        \n",
    "        maxInterval = getattr(maxDelta, dateUnit)//dateInterval*dateInterval\n",
    "       \n",
    "        if reqInterval>maxInterval:\n",
    "            reqInterval = int(maxInterval)\n",
    "        else:\n",
    "            reqInterval = int(reqInterval) #change to integers\n",
    "            \n",
    "        if dateUnit=='days':\n",
    "            if dateInterval==1 and self.start_date != self.end_date:\n",
    "                tmp_end_date = self.start_date + relativedelta(**{dateUnit: reqInterval})\n",
    "            else:\n",
    "                tmp_end_date = self.start_date + relativedelta(**{dateUnit: reqInterval-1})\n",
    "        else:\n",
    "            tmp_end_date = self.start_date + relativedelta(**{dateUnit: reqInterval, \"days\": -1})\n",
    "        if tmp_end_date<self.start_date:\n",
    "            print('Inappropriate start date, date interval, and end date combination')\n",
    "            sys.exit(3)\n",
    "        elif tmp_end_date==self.start_date:\n",
    "            if dateInterval != 1:\n",
    "                print('Inappropriate start date, date interval, and end date combination')\n",
    "                sys.exit(3)\n",
    "            elif dateUnit != 'days':\n",
    "                print('Inappropriate start date, date interval, and end date combination')\n",
    "                sys.exit(3)\n",
    "                \n",
    "        if tmp_end_date != self.end_date:\n",
    "            self.end_date = tmp_end_date\n",
    "            print('End date is changed to {:s} in order to accomodate with date interval of {:d} {:s}' \\\n",
    "                  .format(self.end_date.strftime('%Y-%m-%d'), self.dateInterval, self.dateUnit))\n",
    "            \n",
    "        print('Start date: {:s}, End date: {:s} with interval of {:d} {:s}' \\\n",
    "              .format(self.start_date.strftime('%Y-%m-%d'), self.end_date.strftime('%Y-%m-%d'), \\\n",
    "                      self.dateInterval, self.dateUnit))\n",
    "        \n",
    "        #Get the target dates for the reports\n",
    "        date_delta = self.end_date - self.start_date\n",
    "        self.targeted_dates = [self.start_date+relativedelta(days=i) for i in range(date_delta.days+1)]   \n",
    "    \n",
    "    def check_n_load_data(self,spark_session,target_fields):\n",
    "        \"\"\"\n",
    "            Description: Get the file paths and load the data from HDFS\n",
    "            Purpose: Check if files exist and load the data within the desired date range for the specified orgs\n",
    "            Parameters:\n",
    "                spark_session: spark session instance\n",
    "                target_fields: list of strings; the data fields to load from logs\n",
    "            Return:\n",
    "                A spark dataframe for the date range and specified orgs\n",
    "        \"\"\"\n",
    "        \n",
    "        tmp_paths = []\n",
    "\n",
    "        if self.log_label in ['aracslog', 'arvelog', 'areslog', 'aracsstat']:\n",
    "            path_label = self.log_label + '/*'\n",
    "        else:\n",
    "            path_label = self.log_label\n",
    "            \n",
    "        if self.extra_dates == None:\n",
    "            self.extra_dates = self.targeted_dates\n",
    "        \n",
    "        if self.end_date == date.today()-relativedelta(days=6):\n",
    "            # date_delta = self.end_date - self.start_date\n",
    "            path_dates = self.extra_dates\n",
    "        else: #extend one day later to include small amount of transactions appearing in the next day data\n",
    "            # date_delta = self.end_date + relativedelta(days=1) - self.start_date\n",
    "            path_dates = self.extra_dates + [self.end_date + relativedelta(days=1)]\n",
    "        #path_dates = [self.start_date+relativedelta(days=i) for i in range(date_delta.days+1)]\n",
    "        \n",
    "        #function to check if a file exist in hdfs\n",
    "        #this is needed because there are some data missing for some dates in the hdfs system\n",
    "        #spark: spark session\n",
    "        #path: path pointing to a file in HDFS\n",
    "        def hdfs_check_file(spark, path):\n",
    "            sc = spark.sparkContext\n",
    "            URI           = sc._gateway.jvm.java.net.URI\n",
    "            Path          = sc._gateway.jvm.org.apache.hadoop.fs.Path\n",
    "            FileSystem    = sc._gateway.jvm.org.apache.hadoop.fs.FileSystem\n",
    "            Configuration = sc._gateway.jvm.org.apache.hadoop.conf.Configuration\n",
    "\n",
    "            fs = FileSystem.get(URI(\"hdfs://dfm-cluster\"), Configuration())\n",
    "            status=fs.exists(Path(path))\n",
    "            \n",
    "            return status\n",
    "        \n",
    "        for path_date in path_dates:\n",
    "            tmp_path = \"hdfs://dfm-cluster/DFM/\"+path_label+\"/\" \\\n",
    "                        +\"/\".join(path_date.strftime(\"%Y-%m-%d\").split(\"-\")) \\\n",
    "                        +\"/\"+self.log_label+\".avro\"\n",
    "            status = hdfs_check_file(spark_session, tmp_path)\n",
    "            if status:                    \n",
    "                tmp_paths.append(tmp_path)\n",
    "            else:\n",
    "                print('/'.join(tmp_path.split('/')[-4:]) + ' is missing in HDFS!')\n",
    "        \n",
    "        if 'DATELOGGED' not in target_fields:\n",
    "            target_fields.append('DATELOGGED')\n",
    "        if 'ORGNAME' not in target_fields:\n",
    "            target_fields.append('ORGNAME')\n",
    "                \n",
    "        #Date fields needed for generating the golive report\n",
    "        spark_df=spark_session.read.format(\"com.databricks.spark.avro\").load(tmp_paths) \\\n",
    "                    .filter(sal_process_orgname_udf(F.col(\"ORGNAME\")).isin(sum(self.issuer_sets,[]))) \\\n",
    "                    .withColumn('DATE', F.to_date('DATELOGGED')) \\\n",
    "                    .filter(F.col('DATE').isin(self.extra_dates)) \\\n",
    "                    .select(target_fields) \\\n",
    "\n",
    "        #need to repartition the spark data set to make sure it is not on a single node\n",
    "        numParts = spark_df.rdd.getNumPartitions()\n",
    "        spark_df = spark_df.repartition(int(2*numParts))\n",
    "#         spark_df.cache()\n",
    "#         self.spark_df = spark_df        \n",
    "        return spark_df\n",
    "        print(\"Number of Partitions: \" + str(spark_df.rdd.getNumPartitions())) \n",
    "        \n",
    "    def binField(self, spark_df=None, field=None):\n",
    "        \"\"\"\n",
    "            Description: A general method to bin a field\n",
    "            Purpose: Group and get the count of specified field for each unique value\n",
    "            Parameters:\n",
    "                spark_df: the spark dataframe loaded from avro log files\n",
    "                field: string; the data field the operation will perform on\n",
    "        \"\"\"\n",
    "        try:\n",
    "            tmp_list = spark_df.select(field).groupby(field).count().rdd.map(lambda x: [x[0], x[1]]).collect()\n",
    "            self.value_dist = dict(tmp_list)\n",
    "            self.bin_lower_bound = list(self.value_dist.keys())\n",
    "        except:\n",
    "            if field==None:\n",
    "                print('The field for binning is not given!')\n",
    "            if spark_df==None:\n",
    "                print('The spark dataframe is not given or empty!')\n",
    "            if field not in spark_df.columns:\n",
    "                print('The given field {:s} is not presented in the spark dataframe'.format(field))    \n",
    "    \n",
    "    def check_n_load_metric_data(self, org, metric, field=None):\n",
    "        \"\"\"\n",
    "            Description: A general method to check and load the saved intermediate result\n",
    "            Purpose: 1) Check the saved intermediate result and return the data that already exist;\n",
    "                     2) Determine the extra dates to load data from spark\n",
    "            Parameters:\n",
    "                org: string; the issuer organization name\n",
    "                metric: string; the metric name\n",
    "                field: None (default) or string; the field name for the metric\n",
    "            Return:\n",
    "                The pandas dataframe of the existing data if exists otherwise None\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            metrDset = pd.read_hdf('dfm_metrics.hdf5', '{:s}/{:s}'.format(org, metric))\n",
    "            for dstr in metrDset['Date'].unique():\n",
    "                existing_dates.append(datetime.strptime(dstr, '%Y-%m-%d'))\n",
    "            self.extra_dates = list(set(self.targeted_dates) - set(existing_dates)) #extra dates to load data\n",
    "            existing_dates = set(self.targeted_dates) & set(existing_dates) #metric data already available on these dates\n",
    "            if field==None\n",
    "                return metrDset.loc[pd.to_datetime(metrDset['Date'],'%Y-%m-%d').isin(existing_dates), :]\n",
    "            else:\n",
    "                return metrDset.loc[pd.to_datetime(metrDset['Date'],'%Y-%m-%d').isin(existing_dates),['Date', field+'_value',field+'_count']]\n",
    "        except:\n",
    "            self.extra_dates = self.targeted_dates\n",
    "            return None        \n",
    "    \n",
    "    def save_n_update_metric_data(self, new_data, org, metric):\n",
    "        \"\"\"\n",
    "            Description: Save\n",
    "        \"\"\"\n",
    "        \n",
    "        hdf = pd.HDFStore('dfm_metrics.hdf5', mode='a')\n",
    "        storeMetric = '{:s}/{:s}'.format(org, metric)\n",
    "        hdf.put(storeMetric, new_data, format='table', append=True)   \n",
    "        hdf.close()\n",
    "    \n",
    "    def aggregate_results(self, field, existing_data, new_data):\n",
    "        all_data = pd.concat([existing_data, new_data],axis=0).reset_index()\n",
    "        all_data = all_data.groupby(field+'_value',as_index=False).agg(sum)\n",
    "        self.value_dist = dict([[all_data.loc[i,field+'_value'], all_data.loc[i,field+'_count']] for i in range(len(all_data))])\n",
    "        \n",
    "        if self.bin_lower_bound == None:\n",
    "            self.bin_lower_bound = list(self.value_dist.keys)\n",
    "        \n",
    "    def rtfOutput (self,rtf):\n",
    "        \"\"\"\n",
    "            Description: A centralized, complex, and powerful function to generate tables \n",
    "                        and figures in rtf\n",
    "            Purpose: Make tables and figures for the given attributes to the class instance\n",
    "                    and the upated attributes from computations using spark dataframe\n",
    "            Parameters:\n",
    "                rtf: rtf instance from RTFReport\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.nCalls == None:\n",
    "            if sys.version_info[0] == 3: #python 3\n",
    "                self.nCalls = sum(self.value_dist.values())\n",
    "            else:\n",
    "                self.nCalls = sum(self.value_dist.itervalues())\n",
    "          \n",
    "        if (self.nCalls > 0):\n",
    "            total = self.nCalls\n",
    "\n",
    "            rtf.AddHeading1 (\"Table of \" + self.name)\n",
    "            \n",
    "            if self.description != None:   \n",
    "                description = []\n",
    "                for descrp in self.description:\n",
    "                    description = description + [\"  \" + descrp] + [rtf.newline]\n",
    "                description.pop()\n",
    "                rtf.AddParagraph (*description)\n",
    "            if self.tableColPercent == None:\n",
    "                rtf.TableInit (self.colNames)\n",
    "            else:\n",
    "                numCols = len(self.colNames)\n",
    "                numPers = len(self.tableColPercent)\n",
    "                if numPers > numCols:\n",
    "                    self.tableColPercent = self.tableColPercent[:numCols]\n",
    "                    numPers = numCols\n",
    "                totalPer = sum(self.tableColPercent)\n",
    "                \n",
    "                if totalPer>100:\n",
    "                    self.tableColPerct = [100./numCols for i in range(numCols)]\n",
    "                if numPers<numCols:\n",
    "                    self.tableColPercent = self.tableColPercent + [(100.-totalPer)/(numCols-numPers) for i in range(numCols-numPers)]\n",
    "                rtf.TableInit (self.colNames, T)\n",
    "\n",
    "            cum = 0\n",
    "            x,y,y2 = [], [], []\n",
    "            num_positive = 0\n",
    "            if (self.sortHighToLow):\n",
    "                rng = range (len(self.bin_lower_bound) - 1,0 - 1,-1)\n",
    "            elif (self.sortByFreq):\n",
    "                rng = range (0,len(self.bin_lower_bound))\n",
    "                rng = sorted(range(len(rng)), key=lambda k:rng[k])\n",
    "            else:\n",
    "                rng = range (0,len(self.bin_lower_bound))\n",
    "\n",
    "            for i in rng:\n",
    "                bn = self.bin_lower_bound[i]\n",
    "                if bn not in self.value_dist:\n",
    "                    self.value_dist[bn] = 0\n",
    "\n",
    "                pct = 100.*self.value_dist[bn]/total\n",
    "                cum += self.value_dist[bn]\n",
    "                cum_pct = 100.*cum/total\n",
    "                if isinstance(bn, (int, float)):\n",
    "                    tabCols = [(\"{0:12.\" + str(self.decimals) + \"f}\").format(bn), '{0:,d}'.format(self.value_dist[bn]), '{0:7.2f}%'.format(pct), '{0:,d}'.format(cum), '{0:7.2f}%'.format(cum_pct), '{0:7.2f}%'.format(100-cum_pct)] \n",
    "                elif self.dateType:\n",
    "                    if '+' in bn: #date str + values of given field\n",
    "                        dateStr = bn.split('+')[0]\n",
    "                        fieldValue = bn.split('+')[1] if bn.split('+')[1] !='' else 'Missing'\n",
    "                        tabCols = [(\"{0:s}\").format(dateStr), '{0:,d}'.format(self.value_dist[bn]), '{0:7.2f}%'.format(pct), '{0:,d}'.format(cum), '{0:7.2f}%'.format(cum_pct), '{0:7.2f}%'.format(100-cum_pct)] \n",
    "                        tabCols.insert(1, '{}'.format(fieldValue))\n",
    "                    else:\n",
    "                        dateStr = bn\n",
    "                        tabCols = [(\"{0:s}\").format(dateStr), '{0:,d}'.format(self.value_dist[bn]), '{0:7.2f}%'.format(pct), '{0:,d}'.format(cum), '{0:7.2f}%'.format(cum_pct), '{0:7.2f}%'.format(100-cum_pct)] \n",
    "                \n",
    "                    if ':' in dateStr and '-' in dateStr: #date and hours\n",
    "                        dateFormat = '%Y-%m-%d %H:00:00'\n",
    "                    elif ':' in dateStr: #hours only\n",
    "                        dateFormat = '%H:00:00'\n",
    "                    else: #date only\n",
    "                        dateFormat = '%Y-%m-%d'\n",
    "                        \n",
    "                else:\n",
    "                    tabCols = [(\"{0:s}\").format(bn), '{0:,d}'.format(self.value_dist[bn]), '{0:7.2f}%'.format(pct), '{0:,d}'.format(cum), '{0:7.2f}%'.format(cum_pct), '{0:7.2f}%'.format(100-cum_pct)] \n",
    "                \n",
    "                rtf.TableAddRow (tabCols[:len(self.colNames)])\n",
    "                \n",
    "                if self.dateType:\n",
    "                    if '+' in bn: #date str + values of given field\n",
    "                        x.append([datetime.strptime(dateStr, dateFormat), fieldValue])\n",
    "                    else:\n",
    "                        x.append(datetime.strptime(dateStr, dateFormat))\n",
    "                else:\n",
    "                    x.append(bn)\n",
    "                if self.plotCum:\n",
    "                    y.append(cum_pct)\n",
    "                    y2.append(100-cum_pct)\n",
    "                elif self.plotCounts:\n",
    "                    y.append(self.value_dist[bn])\n",
    "                else:\n",
    "                    y.append(pct)\n",
    "                if ((self.plotYLog != None) and isinstance(bn, (int, float)) and (bn > 0)):\n",
    "                    num_positive += self.value_dist[bn]\n",
    "\n",
    "                if (cum == total):\n",
    "                    break\n",
    "\n",
    "            rtf.TableEnd() \n",
    "            \n",
    "            if self.dateType and '+' in bn: #the case for temporal distribution for given field\n",
    "                x = [[x[i][0], x[i][1], y[i]] for i in range(len(y))]\n",
    "                tmpDF = pd.DataFrame(data=x, columns=['Date', 'Value', 'Stat'])  \n",
    "                values = tmpDF['Value'].unique()\n",
    "                tmpDF = tmpDF.groupby('Value')\n",
    "                \n",
    "                for i in range(len(values)):\n",
    "                    value = values[i]\n",
    "                    \n",
    "                    if i==0:\n",
    "                        y = tmpDF.get_group(value)[['Date', 'Stat']].set_index('Date')\n",
    "                    else:\n",
    "                        tmp = tmpDF.get_group(value)[['Date', 'Stat']].set_index('Date')\n",
    "                        y = pd.concat([y, tmp['Stat']], axis=1)\n",
    "                    \n",
    "                    value = '{}'.format(value)\n",
    "                    if value=='':\n",
    "                        value = 'Missing'\n",
    "                    y.rename(columns={'Stat':value}, inplace=True)\n",
    "                \n",
    "                y = y.reset_index()    \n",
    "                x = list(y['Date'])\n",
    "                y = y.iloc[:,1:]   \n",
    "\n",
    "            if (self.plot):\n",
    "                rtf.AddHeading2 (\"Plot of \" + self.name)\n",
    "\n",
    "                fig = plt.figure(figsize=(6, 3.7), dpi=100)\n",
    "                ax = fig.add_subplot(1,1,1)\n",
    "                plt.gca().set_prop_cycle('color', ['blue', 'cyan', 'magenta', 'black', 'pink', 'red', 'green'])\n",
    "                \n",
    "                font0 = FontProperties()\n",
    "                legend_font = font0.copy()\n",
    "                legend_font.set_size('small')\n",
    "\n",
    "                # ll = plt.plot(x, y,  label=\"Score\", color='b', marker='^')\n",
    "                if (self.plotPoints):\n",
    "                    if self.plotCum:\n",
    "                        ll = plt.plot(x, y, 'o')\n",
    "                    else:\n",
    "                        ll = plt.plot(x, y,  '-', marker='o')\n",
    "                else:\n",
    "                    if self.plotCum:\n",
    "                        ll = plt.plot(x, y, '-')\n",
    "                    else:\n",
    "                        ll = plt.plot(x, y, '-')\n",
    "                        \n",
    "                if self.dateType: \n",
    "                    #keep the number of xticks to 9 or less\n",
    "                    if len(x)>9:\n",
    "                        inc = int(np.ceil(len(x)/8))\n",
    "                        if (len(x)-1)%inc==0:\n",
    "                            self.plotXRange = x[0::inc]\n",
    "                        else:\n",
    "                            self.plotXRange = x[0::inc] + [x[-1]]\n",
    "                    else:\n",
    "                        self.plotXRange = x\n",
    "                        \n",
    "                    if len(x)>48 or dateStr.find(':')==-1: #Too many entries or only date but not hours appear\n",
    "                        ax.xaxis.set_major_formatter(DateFormatter ('%b-%d'))\n",
    "                        ax.fmt_xdata = DateFormatter ('%M-%d')\n",
    "                    elif dateStr.find('-')!=-1: #when there is date in the datetime string\n",
    "                        ax.xaxis.set_major_formatter(DateFormatter ('%b-%d:%H'))\n",
    "                        ax.fmt_xdata = DateFormatter ('%M-%d:%H')\n",
    "                    else: #only hours in the datetime string\n",
    "                        ax.xaxis.set_major_formatter(DateFormatter ('%H:00:00'))\n",
    "                        ax.fmt_xdata = DateFormatter ('%H:00:00')\n",
    "                    plt.xticks(rotation='vertical')\n",
    "                    \n",
    "                    if len(ll)>1: #there are time series for more than one value, need legend\n",
    "                        ax.legend(iter(ll), list(y.columns))\n",
    "                        \n",
    "                # plt.set_title('Score Distribution', fontsize=12)\n",
    "                # plt.grid(b=True, which='both', color='c', linestyle=':', linewidth=1)\n",
    "                plt.grid(True, linestyle=':')\n",
    "                plt.xlabel(self.plotXLabel,  fontsize=12)\n",
    "\n",
    "                if self.plotYLabel != None:\n",
    "                    plt.ylabel(self.plotYLabel, fontsize=12)\n",
    "                else:\n",
    "                    if self.plotCum:\n",
    "                        if self.sortHighToLow:\n",
    "                            plt.ylabel('Cumulative Percent at or Above', fontsize=12)\n",
    "                        else:\n",
    "                            plt.ylabel('Cumulative Percent at or Below', fontsize=12)\n",
    "                    else:\n",
    "                        plt.ylabel('Percent of Trans', fontsize=12)\n",
    "\n",
    "                if self.plotXLog:\n",
    "                    try:\n",
    "                        plt.xscale('log')\n",
    "                    except:\n",
    "                        sys.stderr.write ('WARNING: {0} Could not log-scale the x-axis (pos 1)! {1}\\n'.format(self.name,os.getcwd()))\n",
    "\n",
    "                if (self.plotXRange != None):\n",
    "                    try:\n",
    "                        plt.xlim(self.plotXRange[0], self.plotXRange[-1])\n",
    "                        plt.xticks(self.plotXRange)\n",
    "                    except:\n",
    "                        sys.stderr.write ('WARNING: {0} Could not set limits for log-scale the x-axis! {1}\\n'.format(self.name,os.getcwd()))\n",
    "\n",
    "                if self.plotYLog:\n",
    "                    if (num_positive > 0):\n",
    "                        try:\n",
    "                            plt.yscale('log')\n",
    "                        except:\n",
    "                            sys.stderr.write ('WARNING: {0} Could not log-scale the y-axis (pos 0)! {1}\\n'.format(self.name,os.getcwd()))\n",
    "                            if self.description != None:\n",
    "                                print(\"  \" + self.description)\n",
    "                    else:\n",
    "                        sys.stderr.write ('WARNING: {0} Could not log-scale the y-axis (pos 1)! {1}\\n'.format(self.name,os.getcwd()))\n",
    "                        if self.description != None:\n",
    "                            print(\"  \" + self.description)\n",
    "\n",
    "                if self.plotYRange == None:\n",
    "                    maxY = np.nanmax(y)\n",
    "                    if maxY<10:\n",
    "                        maxY = int(np.ceil(maxY))\n",
    "                        self.plotYRange = range(0,maxY+1, 1)\n",
    "                    else:\n",
    "                        inc = int(np.ceil(maxY/5))\n",
    "                        maxY = inc*5\n",
    "                        self.plotYRange = range(0, maxY+1, inc)                \n",
    "                \n",
    "                try:\n",
    "                    plt.ylim(self.plotYRange[0], self.plotYRange[-1]*1.05)\n",
    "                    plt.yticks(self.plotYRange)\n",
    "                except:\n",
    "                    sys.stderr.write ('WARNING: {0} Could not log-scale the y-axis (pos 2)! {1}\\n'.format(self.name,os.getcwd()))\n",
    "\n",
    "                # plt.set_xticks(range(0,1000,50))\n",
    "                # plt.set_xticklabels(range(0,1000,50), rotation=90, fontsize=10)\n",
    "                plt.tick_params(axis='both', which='major', labelsize=9)\n",
    "                # plt.legend(loc=1,prop={'size':8})\n",
    "\n",
    "                # plt.subplots_adjust(bottom=0.12)  # Stop cutting off the bottom of the label\n",
    "                plt.subplots_adjust(bottom=0.20, left=0.20)  # Stop cutting off the bottom of the label\n",
    "                # (fd, fname) = tempfile.mkstemp(suffix='.png', prefix='tmp', dir=None, text=False)\n",
    "                fd = tempfile.NamedTemporaryFile(suffix='.png', prefix='tmp')\n",
    "                fname = fd.name\n",
    "                try:\n",
    "                    fig.savefig(fname, dpi=80)\n",
    "                    rtf.AddImage (fname)\n",
    "                except:\n",
    "                    sys.stderr.write ('WARNING: {0} Could not save figure to {1}! {2}\\n'.format(self.name,fname,os.getcwd()))\n",
    "\n",
    "            ####################################\n",
    "\n",
    "                if self.plotCum and self.plotInverseCum:\n",
    "                    rtf.AddHeading2 (\"Inverse Plot of \" + self.name)\n",
    "\n",
    "                    fig = plt.figure(figsize=(6, 3.7), dpi=100)\n",
    "                    font0 = FontProperties()\n",
    "                    legend_font = font0.copy()\n",
    "                    legend_font.set_size('small')\n",
    "\n",
    "                    if (self.plotPoints):\n",
    "                        ll = plt.plot(x, y, 'b-', marker='o')\n",
    "                    else:\n",
    "                        ll = plt.plot(x, y2, 'b-')\n",
    "\n",
    "                    plt.grid(True, linestyle=':')\n",
    "                    plt.xlabel(self.plotXLabel,  fontsize=12)\n",
    "\n",
    "                    if self.plotYLabel != None:\n",
    "                        plt.ylabel(\"Inverse of {0:s}\".format(self.plotYLabel), fontsize=12)\n",
    "                    else:\n",
    "                        if self.sortHighToLow:\n",
    "                            plt.ylabel('Cumulative Percent Below', fontsize=12)\n",
    "                        else:\n",
    "                            plt.ylabel('Cumulative Percent Above', fontsize=12)\n",
    "\n",
    "                    if self.plotXLog:\n",
    "                        plt.xscale('log')\n",
    "\n",
    "                    if (self.plotXRange != None):\n",
    "                        plt.xlim(self.plotXRange[0], self.plotXRange[-1])\n",
    "                        plt.xticks(self.plotXRange)\n",
    "\n",
    "                    if self.plotYLog:\n",
    "                        plt.yscale('log')\n",
    "\n",
    "                    if (self.plotYRange != None):\n",
    "                        plt.ylim(self.plotYRange[0], self.plotYRange[-1])\n",
    "                        plt.yticks(self.plotYRange)\n",
    "\n",
    "                    # plt.set_xticks(range(0,1000,50))\n",
    "                    # plt.set_xticklabels(range(0,1000,50), rotation=90, fontsize=10)\n",
    "                    plt.tick_params(axis='both', which='major', labelsize=9)\n",
    "                    # plt.legend(loc=1,prop={'size':8})\n",
    "\n",
    "                    # plt.subplots_adjust(bottom=0.12)  # Stop cutting off the bottom of the label\n",
    "                    plt.subplots_adjust(bottom=0.20)  # Stop cutting off the bottom of the label\n",
    "                    # (fd, fname) = tempfile.mkstemp(suffix='.png', prefix='tmp', dir=None, text=False)\n",
    "                    fd = tempfile.NamedTemporaryFile(suffix='.png', prefix='tmp')\n",
    "                    fname = fd.name\n",
    "                    try:\n",
    "                        fig.savefig(fname, dpi=80)\n",
    "                        rtf.AddImage (fname)\n",
    "                    except:\n",
    "                        print('WARNING: {0} Could not save figure to {1}!'.format(self.name,fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A subclass of GoLiveReport to make statistics on date or hours for a given field\n",
    "class TemporalDist(GoLiveReport):\n",
    "    def __init__ (self, name='Dates', start_date=None, dateInterval=1, dateUnit='days', \n",
    "                  tableColPercent = [28, 18, 18, 18, 18], plot=False, plotXLabel=None,\n",
    "                  plotCounts=True\n",
    "            ):\n",
    "        \n",
    "        if plotCounts:\n",
    "            plotYLabel = 'Transaction count'\n",
    "        else:\n",
    "            plotYLabel = 'Percent of transactions'\n",
    "        \n",
    "        GoLiveReport.__init__(self, name, dateType=True, plot=plot, plotPoints=True, plotCounts=plotCounts, \n",
    "                             plotCum=False, tableColPercent=tableColPercent, plotYLabel=plotYLabel)\n",
    "        \n",
    "        if start_date == None:\n",
    "            self.start_date = datetime.today().date()\n",
    "        else:\n",
    "            self.start_date = start_date\n",
    "        \n",
    "        if 'HOUR'==name.upper() or 'HOUR' in name.upper(): #time series related with hour of the day   \n",
    "            self.colNames = ['Hour of Day bin (UTC)', \"Count\", \"Percent\", \"Cumulative\", \"Cum_Pct\"]\n",
    "            self.dateInterval = dateInterval\n",
    "            self.dateUnit = 'hours'\n",
    "            self.aggUnit = 'hours'\n",
    "        else:\n",
    "            self.dateUnit = dateUnit.lower()\n",
    "            if dateUnit.lower()=='weeks':\n",
    "                self.name = 'Transactions in every {:d} week(s)'.format(dateInterval)\n",
    "                self.dateInterval = int(7*dateInterval)\n",
    "                self.aggUnit = 'weeks'\n",
    "                self.dateUnit = 'days'\n",
    "            elif dateUnit.lower()=='months':\n",
    "                self.name = 'Transactions in every {:d} month(s)'.format(dateInterval)\n",
    "                self.dateInterval = int(dateInterval)\n",
    "                self.aggUnit = 'months'\n",
    "            elif dateUnit.lower()=='quarters': \n",
    "                self.name = 'Transactions in every {:d} quarter(s)'.format(dateInterval)\n",
    "                self.dateInterval = 3*dateInterval\n",
    "                self.aggUnit = 'quarters'\n",
    "                self.dateUnit = 'months'\n",
    "            elif dateUnit.lower()=='years':\n",
    "                self.name = 'Transactions in every {:d} year(s)'.format(dateInterval)\n",
    "                self.dateInterval = dateInterval\n",
    "                self.aggUnit = 'years'\n",
    "            elif dateUnit.lower()=='days':\n",
    "                self.name = 'Transactions in every {:d} day(s)'.format(dateInterval)\n",
    "                self.dateInterval = dateInterval\n",
    "                self.aggUnit = 'days'\n",
    "            else:\n",
    "                print('WARNING: Cannot parse the date unit. Pass days, weeks, months, quarters, and years only.')\n",
    "                sys.exit(1)\n",
    "            self.colNames = ['Date bin lower bound (UTC)', \"Count\", \"Percent\", \"Cumulative\", \"Cum_Pct\"]\n",
    "    \n",
    "\n",
    "    #get the date string from the datetime string in the DATELOGGED field\n",
    "    @staticmethod\n",
    "    def get_date_str_udf(start_date, interval=1, unit='days'):\n",
    "        def get_date_str(s, start_date, interval, unit):\n",
    "            if unit.lower()=='days':\n",
    "                delta = s - start_date\n",
    "            else:\n",
    "                delta = relativedelta(s, start_date)\n",
    "            interval = getattr(delta, unit.lower())//interval*interval\n",
    "            dstr = start_date + relativedelta(**{unit: interval})\n",
    "            return datetime.strftime(dstr, '%Y-%m-%d')            \n",
    "            \n",
    "        return F.udf(lambda x: get_date_str(x, start_date, interval, unit), StringType())\n",
    "\n",
    "    #get the hour string from the datetime string in the DATELOGGED field\n",
    "    @staticmethod\n",
    "    def get_hour_str_udf():\n",
    "        def get_hour_str(s):\n",
    "            if '+' in s: #'+' may appear to show the timezone info but it is not necessary as normally it is \"+0000\"\n",
    "                idx_plus = s.find('+')\n",
    "                s = s[:idx_plus]\n",
    "            s = s.replace('T', ' ')\n",
    "            s = datetime.strptime(s, '%Y-%m-%d %H:%M:%S')\n",
    "            hour = datetime.strftime(s, '%Y-%m-%d %H:00:00').split()[1]\n",
    "            return hour\n",
    "        return F.udf(get_hour_str, StringType())\n",
    "        \n",
    "    def binField(self, spark_df, field=None):  \n",
    "        start_date = self.start_date\n",
    "        dateInterval = self.dateInterval\n",
    "        unit = self.dateUnit\n",
    "        \n",
    "        if self.dateUnit=='hours':\n",
    "            hour_str_udf = self.get_hour_str_udf()\n",
    "            \n",
    "            spark_df = spark_df.withColumn('datestr', hour_str_udf(F.col('DATELOGGED')))\n",
    "            if spark_df.select('DATE').distinct().count()==1:\n",
    "                spark_df = spark_df.withColumn('datestr', F.concat(F.date_format('DATE', 'yyyy-MM-dd'), F.lit(' '), F.col('datestr')))\n",
    "            description = [\"N.B.: Hour bin is the lower bin boundary.\", \\\n",
    "                           \"The date and time are in UTC timeszone, and the total number of {1:s} is: {0:8,d}\"\n",
    "                          ]\n",
    "        else:   \n",
    "            date_str_udf = self.get_date_str_udf(start_date, interval=dateInterval, unit=unit)\n",
    "            spark_df = spark_df.withColumn('datestr', date_str_udf(F.col('DATE')))                \n",
    "        \n",
    "            description = [\"N.B.: Date bin is the lower bin boundary.\", \\\n",
    "                           \"The date is based upon UTC timeszone, and the total number of {1:s} is: {0:8,d}\"\n",
    "                          ]\n",
    "        if field != None:\n",
    "            tmp_list = spark_df.select('datestr', field).groupby(['datestr', field]).count().rdd.map(lambda x: [x[0], x[1], x[2]]).collect()\n",
    "            self.colNames = [\"Bin of {:s} lower bound (UTC)\".format(self.aggUnit), field, \"Count\", \"Percent\", \"Cumulative\", \"Cum_Pct\"]\n",
    "            self.tableColPercent = [26, 22, 12, 13, 14, 13]\n",
    "        else:\n",
    "            tmp_list = spark_df.select('datestr').groupby('datestr').count().rdd.map(lambda x: [x[0], x[1]]).collect()\n",
    "        \n",
    "        numDates = spark_df.select('datestr').distinct().count()\n",
    "        description[1] = description[1].format(numDates, self.aggUnit)\n",
    "        self.description = description\n",
    "\n",
    "        allList = []\n",
    "        for tmp in tmp_list:\n",
    "            dt = tmp[0]           \n",
    "            if len(dt)>8:\n",
    "                if dt.find(':')==-1: #only date\n",
    "                    dateFormat = \"%Y-%m-%d\"                    \n",
    "                else: #date and time\n",
    "                    dateFormat = \"%Y-%m-%d %H:00:00\"\n",
    "            else: #only time\n",
    "                dateFormat = \"%H:00:00\"\n",
    "\n",
    "            bn=datetime.strptime(dt, dateFormat)\n",
    "            if field != None:\n",
    "                allList.append([bn, tmp[1], tmp[2]])\n",
    "            else:\n",
    "                allList.append([bn, tmp[1]])\n",
    "        allList = sorted(allList, key=lambda x: (x[0], x[1]))\n",
    "\n",
    "        #create the bins and dictionary to store the counts for each bin\n",
    "        self.bin_lower_bound = [] \n",
    "        tmp_list = []\n",
    "        for dst in allList:           \n",
    "            if field != None:\n",
    "                key = \"{0}+{1}\".format(dst[0].strftime(dateFormat),dst[1])\n",
    "                tmp_list.append([key, dst[2]])\n",
    "            else:\n",
    "                key = dst[0].strftime(dateFormat)\n",
    "                tmp_list.append([key, dst[1]])\n",
    "            self.bin_lower_bound.append(key)\n",
    "        self.value_dist = dict(tmp_list)\n",
    "        # value_dist = dict(tmp_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A subclass of GoLiveReport to make Score Distribution\n",
    "class ScoreDist(GoLiveReport):\n",
    "    def __init__ (self, name='Score', bins=range(0,1001,50), description=None, \n",
    "                plotXRange=range(0,1001,200), plotYRange=range(0,101,20), plotYLabel=None\n",
    "                ):\n",
    "        \n",
    "        if bins[0]>0: \n",
    "            plotInverseCum = False\n",
    "            if name==None:\n",
    "                name = 'Score (low outsort range)'\n",
    "            if description == None:\n",
    "                description = [\"N.B.: bin is the lower bin boundary\", \\\n",
    "                           \"The score distribution for the high score and low outsort range.\"]\n",
    "        else:\n",
    "            plotInverseCum = True\n",
    "            if description == None:\n",
    "                description = [\"N.B.: bin is the lower bin boundary\", \\\n",
    "                           \"The score distribution for the entire score range.\"]\n",
    "            \n",
    "        if plotXRange == None:\n",
    "            plotXRange = bins;  \n",
    "        \n",
    "        if plotYLabel == None:\n",
    "            plotYLabel='Transaction Outsort Rate (%)'\n",
    "        \n",
    "        GoLiveReport.__init__(self, name, bins=bins, description=description, sortHighToLow=True, \n",
    "                              decimals=0, plotCum=True, plotInverseCum=plotInverseCum, plotXRange=plotXRange, \n",
    "                              plotYRange=plotYRange, plotXLabel='Score Threshold', plotYLabel=plotYLabel\n",
    "                             )\n",
    "        self.bins = bins\n",
    "    \n",
    "    def binField(self, spark_df, card_max=False):    \n",
    "        increment = (max(self.bins) - min(self.bins))//(len(self.bins)-1)\n",
    "        lowest = min(self.bins)\n",
    "     \n",
    "        def binned(fd, increment=increment, lowest=lowest):    \n",
    "            if fd==-999:\n",
    "                return -999\n",
    "\n",
    "            if fd<lowest:\n",
    "                return -1;\n",
    "            else:\n",
    "                return int(fd//increment*increment)\n",
    "        binned_udf = F.udf(binned, IntegerType())\n",
    "        \n",
    "        spark_df=spark_df.select('HASHCN', 'PREDICTIVE_SCORE', 'MODEL_PRIMING_SCORE') \\\n",
    "                    .withColumn('SCORE', F.when(F.col('PREDICTIVE_SCORE')==-999, \\\n",
    "                                            F.when(F.col('MODEL_PRIMING_SCORE')!='', \\\n",
    "                                            F.col('MODEL_PRIMING_SCORE').astype('integer')) \\\n",
    "                                            .otherwise(-999)).otherwise(F.col('PREDICTIVE_SCORE')))\n",
    "        \n",
    "        if card_max: #get the max score distribution for the card\n",
    "            spark_df = spark_df.groupby('HASHCN').agg(F.max('SCORE').alias('SCORE'))\n",
    "\n",
    "        tmp_list = spark_df.select('SCORE').withColumn('binned', binned_udf(F.col('SCORE'))) \\\n",
    "                           .select('binned').groupby('binned') \\\n",
    "                           .count().rdd.map(lambda x: [x[0], x[1]]).collect()\n",
    "        self.value_dist = dict(tmp_list) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NumTxnDist(GoLiveReport):\n",
    "    def __init__(self, name='Number of Transactions Per Card (full score range)', bins=range(1,101)\n",
    "                ):\n",
    "\n",
    "        GoLiveReport.__init__(self, name, bins=bins, decimals=0, plot=True, plotPoints=True,\n",
    "                              plotXLabel='Number of transactions per card', plotYLabel='Percent of cards (%)'\n",
    "                             )\n",
    "        \n",
    "    def binField(self, spark_df):\n",
    "        card_df = spark_df.groupby('HASHCN').agg(F.count('HASHCN').alias('NUMTXN'))\n",
    "        maxTxns = card_df.select(F.max('NUMTXN')).rdd.map(lambda x: x[0]).collect()[0]\n",
    "        \n",
    "        self.bin_lower_bound = range(1,maxTxns+1)\n",
    "        self.description = [\"N.B.: bin is the lower bin boundary\", \\\n",
    "                            \"The number or transactions per card during the time of this report.  Max number of transactions on one card is {0:,d}.\".format(maxTxns)\n",
    "                           ]        \n",
    "\n",
    "        tmp_list = card_df.select('NUMTXN').groupby('NUMTXN') \\\n",
    "                           .count().rdd.map(lambda x: [x[0], x[1]]).collect()\n",
    "        self.value_dist = dict(tmp_list) \n",
    "        \n",
    "        #need to give xticks for plotting, only take eight ticks\n",
    "        increment = int(np.ceil(maxTxns/7))\n",
    "        maxTxns = int(increment*7)\n",
    "        self.plotXRange = range(0, maxTxns+1, increment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CardScoreDist(ScoreDist):\n",
    "    def __init__(self, name='Per-Card Max-Score Distribution (full score range)', bins=range(0,1001,50), \n",
    "                plotXRange=range(0,1001,200), plotYRange=None\n",
    "                ): \n",
    "\n",
    "        if bins[0]>0: \n",
    "            plotInverseCum = False\n",
    "            if name==None:\n",
    "                name = 'Per-Card Max-Score Distribution (full score range)'\n",
    "            description = [\"N.B.: bin is the lower bin boundary\", \\\n",
    "                           \"The max-score-per-card distribution.\"\n",
    "                          ]\n",
    "        else:\n",
    "            plotInverseCum = True\n",
    "            plotYRange = range(0,101, 20)\n",
    "            description = [\"N.B.: bin is the lower bin boundary\", \\\n",
    "                           \"The max-score-per-card distribution for the high score and low outsort range.\"\n",
    "                          ]\n",
    "\n",
    "        ScoreDist.__init__(self, name=name, bins=bins, description=description, \n",
    "                plotXRange=plotXRange, plotYRange=plotYRange, plotYLabel='Card Outsort Rate (%)')\n",
    "\n",
    "    def binField(self, spark_df):\n",
    "        ScoreDist.binField(self, spark_df, card_max=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AmountDist(GoLiveReport):\n",
    "    def __init__(self, name='Amount Bin', bins=None, plotXLabel='Amount (Base Currency)', \n",
    "                 amountField='BASE_CURR_AMOUNT'                \n",
    "                ):\n",
    "        if amountField=='BASE_CURR_AMOUNT':\n",
    "            description = [\"N.B.: bin is the lower bin boundary\", \\\n",
    "                           \"The binned amount in the Base Currency of the portfolio.\"]\n",
    "            plotXLabel = \"Amount (Base Currency)\"\n",
    "        else:\n",
    "            description = [\"N.B.: bin is the lower bin boundary\", \\\n",
    "                           \"The binned amount in USD.\"]\n",
    "            plotXLabel = \"Amount (USD)\"\n",
    "            \n",
    "        self.amountField = amountField\n",
    "        \n",
    "        GoLiveReport.__init__(self, name, bins=bins, description=description, decimals=2, plot=True, plotXLog=True, \n",
    "                              plotXLabel=plotXLabel, plotYLabel='Percent of Trans (%)')\n",
    "        \n",
    "    #transform the amount to desired log scale\n",
    "    @staticmethod\n",
    "    def log_range(min, max, nval):\n",
    "        r = None\n",
    "        r = range (0, nval)\n",
    "        r = [math.exp(math.log(min) + x*(math.log(max)-math.log(min))/(nval-1)) for x in r]\n",
    "        return r\n",
    "    \n",
    "    @staticmethod\n",
    "    def amount_transform_udf(rng):\n",
    "        def amount_transform(amt, rng):\n",
    "            if amt<0.1:\n",
    "                return float(\"-inf\")\n",
    "            else:\n",
    "                for i in range(len(rng)-1):\n",
    "                    if amt>=rng[i] and amt<rng[i+1]:\n",
    "                        return rng[i]\n",
    "            if amt>=rng[-1]:\n",
    "                return rng[-1]\n",
    "            else:\n",
    "                return float(\"-inf\")\n",
    "            \n",
    "        return F.udf(lambda amt: amount_transform(amt, rng), FloatType())  \n",
    "    \n",
    "    def binField(self, spark_df):\n",
    "        rng = self.log_range(0.1, 100000, 25)\n",
    "        if self.bin_lower_bound==None:            \n",
    "            self.bin_lower_bound = list(set([float(\"-inf\")] + rng))\n",
    "        self.bin_lower_bound.sort()\n",
    "        amount_transform_udf = self.amount_transform_udf(rng)\n",
    "        \n",
    "        tmp_list = spark_df.select(self.amountField) \\\n",
    "                           .withColumn('AMOUNT', amount_transform_udf(F.col(self.amountField))) \\\n",
    "                           .groupby('AMOUNT').count().rdd.map(lambda x: [x[0], x[1]]).collect()  \n",
    "                \n",
    "        #need to deal with the precision difference between python and spark\n",
    "        self.bin_lower_bound = list(np.round(self.bin_lower_bound, 2))\n",
    "        tmp_list = [[np.round(x[0],2), x[1]] for x in tmp_list]\n",
    "        \n",
    "        self.value_dist = dict(tmp_list)\n",
    "        \n",
    "        idx = self.bin_lower_bound.index(max(self.value_dist))\n",
    "        self.bin_lower_bound = self.bin_lower_bound[:idx+1]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ErrorDist(GoLiveReport):\n",
    "    def __init__(self, name='Error and Warnings'):\n",
    "        \n",
    "        GoLiveReport.__init__(self, name, colNames=['Value', 'Count', 'Percent'], \n",
    "                              tableColPercent=[60, 20, 20], plot=False)\n",
    "        \n",
    "    #define a udf to parse error code\n",
    "    @staticmethod\n",
    "    def error_parse_udf():\n",
    "        def error_parse(s):\n",
    "            errorStr = re.compile(r\"^error\", re.I)\n",
    "            warnStr = re.compile(r\"^warn\", re.I)\n",
    "            timeStr = re.compile(r\"Timeout\", re.I)\n",
    "\n",
    "            if len(s)>0:\n",
    "                if re.match(errorStr, s):\n",
    "                    return 'Error'\n",
    "                elif re.match(timeStr, s):\n",
    "                    return 'Error'\n",
    "                elif not re.match(warnStr, s):\n",
    "                    return 'Other'\n",
    "                else:\n",
    "                    return 'Warning'\n",
    "            else:\n",
    "                return 'Clean'\n",
    "        return F.udf(error_parse, StringType()) \n",
    "    \n",
    "    def get_error_stat(self, spark_df):\n",
    "        error_parse_udf = self.error_parse_udf()\n",
    "        spark_df = spark_df.select('MODEL_ERROR').withColumn('ERROR_TYPE', error_parse_udf(F.col('MODEL_ERROR')))\n",
    "\n",
    "        tmp = spark_df.groupby('ERROR_TYPE').agg(F.count('ERROR_TYPE').alias('ERR_COUNT'))\n",
    "        try:\n",
    "            nErrors = tmp.filter(F.col('ERROR_TYPE')=='Error').rdd.map(lambda x: x[1]).collect()[0]\n",
    "        except:\n",
    "            nErrors = 0\n",
    "        try:\n",
    "            nWarnings = tmp.filter(F.col('ERROR_TYPE')=='Warning').rdd.map(lambda x: x[1]).collect()[0]\n",
    "        except:\n",
    "            nWarnings = 0\n",
    "        try:\n",
    "            nOther = tmp.filter(F.col('ERROR_TYPE')=='Other').rdd.map(lambda x: x[1]).collect()[0]\n",
    "        except:\n",
    "            nOther = 0\n",
    "        try:\n",
    "            nClean = tmp.filter(F.col('ERROR_TYPE')=='Clean').rdd.map(lambda x: x[1]).collect()[0]\n",
    "        except:\n",
    "            nClean = 0\n",
    "        \n",
    "        total = nErrors + nWarnings + nOther\n",
    "        nCalls = total + nClean\n",
    "        \n",
    "        #Need to pass the total to super class\n",
    "        self.nCalls = nCalls\n",
    "          \n",
    "        self.description = [\"Total errors or warnings: {0:,d}\".format (total),\n",
    "                           \"Total calls: {0:,d}\".format (nCalls),\n",
    "                           \"Transactions with errors:  \\t {0:13,d}\".format (nErrors),\n",
    "                           \"Transactions with warnings:\\t {0:13,d}\".format (nWarnings),\n",
    "                           \"Transactions with other:   \\t {0:13,d}\".format (nOther), \n",
    "                           \"Basis-points of errors and warnings: {0:7.2f}\".format (100*100*(1. - nClean*1./nCalls))\n",
    "                           ]\n",
    "    \n",
    "    def binField(self, spark_df):\n",
    "        self.get_error_stat(spark_df)\n",
    "        GoLiveReport.binField(self, spark_df.filter(F.col('MODEL_ERROR')!=''), 'MODEL_ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BaseCurrDist(GoLiveReport):\n",
    "    def __init__(self, name='BaseCurrCode'):\n",
    "        colNames = [\"Value\", \"Count\", \"Percent\", \"Cumulative\", \"Cum_Pct\"]\n",
    "        GoLiveReport.__init__(self, name, decimals=0, colNames=colNames, tableColPercent=[36], plot=False)    \n",
    "    \n",
    "    def binField(self, spark_df):\n",
    "        self.bin_lower_bound = spark_df.select('BASE_CURR_CODE').distinct().collect()\n",
    "        GoLiveReport.binField(self, spark_df, 'BASE_CURR_CODE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GroupDist(GoLiveReport):\n",
    "    def __init__(self, name='Group'):\n",
    "        colNames = [\"Value\", \"Count\", \"Percent\", \"Cumulative\", \"Cum_Pct\"]\n",
    "        GoLiveReport.__init__(self, name, colNames=colNames, tableColPercent=[36], plot=False)\n",
    "    \n",
    "    def binField(self, spark_df):\n",
    "        GoLiveReport.binField(self, spark_df, 'ORGNAME')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelDist(GoLiveReport):\n",
    "    def __init__(self, name='Model'):\n",
    "        colNames = [\"Value\", \"Count\", \"Percent\", \"Cumulative\", \"Cum_Pct\"]\n",
    "        GoLiveReport.__init__(self, name, colNames=colNames, tableColPercent=[36], plot=False)\n",
    "        \n",
    "    def binField(self, spark_df):        \n",
    "        spark_df = spark_df.withColumn('MODEL', F.concat(F.col('MODEL_ID'), F.lit(' v'), F.col('MODEL_VER'))) \\\n",
    "                           .withColumn('MODEL', F.when(F.col('MODEL')==' v', 'N/A').otherwise(F.col('MODEL')))\n",
    "        GoLiveReport.binField(self, spark_df, 'MODEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ScoreTypeDist(GoLiveReport):\n",
    "    def __init__(self, name='Scoring Type'):\n",
    "        colNames = [\"Value\", \"Count\", \"Percent\", \"Cumulative\", \"Cum_Pct\"] \n",
    "        GoLiveReport.__init__(self, name, colNames=colNames, plot=False)\n",
    "    \n",
    "    def binField(self, spark_df):\n",
    "        spark_df = spark_df.withColumn('SCORETYPE', F.when((F.col('PREDICTIVE_SCORE').isNull()) & (F.col('MODEL_PRIMING_SCORE')==''), \\\n",
    "                                            F.lit('Blank')).when(F.col('PREDICTIVE_SCORE')==-999, F.lit('Priming')) \\\n",
    "                                           .otherwise(F.lit('Normal')))\n",
    "        GoLiveReport.binField(self, spark_df, 'SCORETYPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#spark.executor.instances: num-executor; also look at spark.dynamicAllocation.initialExecutors\n",
    "#spark.executor.cores: executor-cores\n",
    "#spark.executor.memory: executor-memory\n",
    "#spark.submit.deployMode: deploy-mode\n",
    "spark = SparkSession.builder \\\n",
    "            .master(\"yarn\") \\\n",
    "            .appName(\"linhai_golive_rept\") \\\n",
    "            .config(\"spark.executor.instances\", 4) \\\n",
    "            .config(\"spark.executor.cores\",4) \\\n",
    "            .config(\"spark.executor.memory\",'4G') \\\n",
    "            .config(\"spark.submit.deployMode\",'client') \\\n",
    "            .config(\"spark.yarn.executor.memoryOverhead\", '2G') \\\n",
    "            .getOrCreate()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = datetime.today().time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start date: 2018-05-11, End date: 2018-05-11 with interval of 1 days\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[ORGNAME: string, HASHCN: string, PREDICTIVE_SCORE: bigint, MODEL_PRIMING_SCORE: string, DATE: date, AMOUNTUSD: double, TXNSTATUS: string, DATELOGGED: string, BASE_CURR_CODE: bigint, BASE_CURR_AMOUNT: double, MODEL_ERROR: string, MODEL_ID: string, MODEL_VER: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_filepath = '/home/lijli06/Work/DFM_SPARK/golive_report/job_config.json'\n",
    "glr = GoLiveReport('Super_Metric')\n",
    "glr.load_job_config(job_filepath)\n",
    "glr.validate_date_range()\n",
    "\n",
    "target_fields = ['ORGNAME', 'HASHCN', 'PREDICTIVE_SCORE', 'MODEL_PRIMING_SCORE', 'DATE', 'AMOUNTUSD','TXNSTATUS', \\\n",
    "                 'DATELOGGED', 'BASE_CURR_CODE', 'BASE_CURR_AMOUNT', 'MODEL_ERROR', 'MODEL_ID', 'MODEL_VER']\n",
    "all_df = glr.check_n_load_data(spark, target_fields)\n",
    "all_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: rescaling /tmp/tmpg82g_pnz.png  by a factor of 0.73\n",
      "Warning: rescaling /tmp/tmp41l7pb5m.png  by a factor of 0.73\n",
      "Warning: rescaling /tmp/tmpcdy3o6nf.png  by a factor of 0.73\n",
      "Warning: rescaling /tmp/tmpu_p0rnny.png  by a factor of 0.73\n",
      "Warning: rescaling /tmp/tmpghnbi6sl.png  by a factor of 0.73\n",
      "Warning: rescaling /tmp/tmp0jvtxexc.png  by a factor of 0.73\n",
      "Warning: rescaling /tmp/tmp96m01zvj.png  by a factor of 0.73\n",
      "Warning: rescaling /tmp/tmp7zpxrcr0.png  by a factor of 0.73\n",
      "Warning: rescaling /tmp/tmp9lcvc_oz.png  by a factor of 0.73\n"
     ]
    }
   ],
   "source": [
    "#generate report for each org\n",
    "currdate = datetime.today().date().strftime(\"%B %d, %Y\")\n",
    "\n",
    "for numIssGrp in range(len(glr.issuer_groups)):\n",
    "    for orgName in glr.issuer_sets[numIssGrp]:\n",
    "        rtfReport = RTFReport.RTFReport(glr.issuer_groups[numIssGrp]+'_'+orgName)\n",
    "        if (orgName.upper() == \"ALL_BANKS\"):\n",
    "            rtfReport.AddTitle ('Report for All Organizations')\n",
    "        else:\n",
    "            rtfReport.AddTitle ('Report for Organization {0:s}'.format(orgName))\n",
    "        rtfReport.AddSubTitle ('Prepared on ' + currdate)\n",
    "        if orgName.upper() == 'ALL_BANKS':\n",
    "            rtfReport.AddParagraph ('This report documents the performance of all portfolios, examining all transactions in Risk Analytics which are eligible to receive a score.')\n",
    "        else:\n",
    "            rtfReport.AddParagraph ('This report documents the performance of the {0:s} portfolio, examining all transactions in Risk Analytics which are eligible to receive a score.'.format(orgName))\n",
    "            banks = [orgName]\n",
    "        \n",
    "        #obtain the data for the specified orgs in the group\n",
    "        tmp_df = all_df.filter(sal_process_orgname_udf(F.col(\"ORGNAME\")).isin(banks))\n",
    "        #numParts = tmp_df.rdd.getNumPartitions()\n",
    "        #print(\"Number of Partitions: \" + str(numParts)) \n",
    "        #tmp_df = tmp_df.repartition(int(2*numParts))\n",
    "        \n",
    "        #Get the statistics for the table of dates\n",
    "        dateDist = TemporalDist(name='Dates', start_date=glr.start_date, plot=False,\n",
    "                                dateInterval=glr.dateInterval, dateUnit=glr.dateUnit);\n",
    "        dateDist.binField(tmp_df)\n",
    "        \n",
    "        #Get the statistics for score type\n",
    "        scoreTypeDist = ScoreTypeDist()\n",
    "        scoreTypeDist.binField(tmp_df)\n",
    "       \n",
    "        #Get the score distribution\n",
    "        scoreDist = ScoreDist()\n",
    "        scoreDist.binField(tmp_df)\n",
    "\n",
    "        #Get the high score distribution\n",
    "        highScoreDist = ScoreDist(name=\"Score (low outsort range)\", bins=range(400,1001,10),\n",
    "                         plotXRange=range(400,1001,100), plotYRange=range(0,6,1))\n",
    "        highScoreDist.binField(tmp_df)\n",
    "        \n",
    "        #Get the number of transactions per card\n",
    "        numTxnDist = NumTxnDist()\n",
    "        numTxnDist.binField(tmp_df)\n",
    "        \n",
    "        #Get the Per-Card Max-Score Distribution (full score range)\n",
    "        maxScoreDist = CardScoreDist()\n",
    "        maxScoreDist.binField(tmp_df)\n",
    "\n",
    "        #Get the Per-Card Max-Score Distribution (high score range) \n",
    "        maxHighScoreDist = CardScoreDist(name='Per-Card Max-Score Distribution (low outsort range)', \n",
    "                                           bins=range(400,1001,10), plotXRange=range(400,1001,100))\n",
    "        maxHighScoreDist.binField(tmp_df)\n",
    "        \n",
    "        #Get the statistics of errors and warnings\n",
    "        errorDist = ErrorDist()\n",
    "        errorDist.binField(tmp_df)\n",
    "        \n",
    "        #Get the statistics of BaseCurrCode\n",
    "        baseCurrDist = BaseCurrDist()\n",
    "        baseCurrDist.binField(tmp_df)\n",
    "\n",
    "        #Get the statistics of amount\n",
    "        if orgName.upper()=='ALL_BANKS':\n",
    "            amountField = 'AMOUNTUSD'\n",
    "        else:\n",
    "            amountField = 'BASE_CURR_AMOUNT'\n",
    "        amountDist = AmountDist(amountField=amountField)\n",
    "        amountDist.binField(tmp_df)\n",
    "        \n",
    "        #Get the hour of the day statistics       \n",
    "        hourDist = TemporalDist(name='Hour of Day', plot=True, plotXLabel='Hour of Day')\n",
    "        hourDist.binField(tmp_df)\n",
    "        \n",
    "        #Get the statistics of the groups\n",
    "        groupDist = GroupDist()\n",
    "        groupDist.binField(tmp_df)\n",
    "        \n",
    "        #Get the statistics of model\n",
    "        modelDist = ModelDist()\n",
    "        modelDist.binField(tmp_df)\n",
    "        \n",
    "        dateDist.rtfOutput(rtfReport)\n",
    "        scoreTypeDist.rtfOutput(rtfReport)\n",
    "        scoreDist.rtfOutput(rtfReport)\n",
    "        highScoreDist.rtfOutput(rtfReport)\n",
    "        numTxnDist.rtfOutput(rtfReport)\n",
    "        maxScoreDist.rtfOutput(rtfReport)\n",
    "        maxHighScoreDist.rtfOutput(rtfReport)\n",
    "        errorDist.rtfOutput(rtfReport)\n",
    "        baseCurrDist.rtfOutput(rtfReport)\n",
    "        amountDist.rtfOutput(rtfReport)\n",
    "        hourDist.rtfOutput(rtfReport)\n",
    "        groupDist.rtfOutput(rtfReport)\n",
    "        modelDist.rtfOutput(rtfReport)\n",
    "        \n",
    "        start_date = glr.start_date\n",
    "        end_date = glr.end_date\n",
    "        if start_date==end_date:\n",
    "            filename = 'Report_{0:s}_{1:s}.rtf'.format(start_date.strftime(\"%Y%m%d\"), orgName.strip())\n",
    "        else:\n",
    "            filename = 'Report_{0:s}_{1:s}_{2:s}.rtf'.format(start_date.strftime(\"%Y%m%d\"), end_date.strftime(\"%Y%m%d\"), orgName.strip())\n",
    "                \n",
    "        rtfReport.Output(filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ORGNAME: string, HASHCN: string, PREDICTIVE_SCORE: bigint, MODEL_PRIMING_SCORE: string, DATE: date, AMOUNTUSD: double, TXNSTATUS: string, DATELOGGED: string, BASE_CURR_CODE: bigint, BASE_CURR_AMOUNT: double, MODEL_ERROR: string, MODEL_ID: string, MODEL_VER: string]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take total 65.71 seconds to finish\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.today().time()\n",
    "elapse_time = datetime.combine(datetime.today(),end_time) - datetime.combine(datetime.today(), start_time)\n",
    "print('Take total {:.2f} seconds to finish'.format(elapse_time.total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([('a', 'aaa'), ('a', 'bbb'), ('a', 'aaa'), ('b', 'aaa'), ('b', 'bbb'), ('b', 'bbb')], ['x', 'y'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.groupby('x').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.groupby(['x', 'y']).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
